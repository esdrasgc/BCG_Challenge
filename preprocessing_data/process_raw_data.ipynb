{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pdfplumber\n",
    "# !pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metadata_pdfs = {\n",
    "    'nacional': {'filename': \"plano-acao-adaptacao-climatica-nacional.pdf\", 'start_page' : 6, 'line_number' : None, 'skip_lines' : 1},\n",
    "    'agro': {'filename' : \"plano-acao-climatica-agro.pdf\", 'start_page' : 20, 'skip_lines' : 1},\n",
    "    'curitiba' : {'filename' : 'plano-acao-climatica-curitiba.pdf', \"start_page\" : 16},\n",
    "    'federal' : {'filename': 'plano-acao-climatica-federal.pdf', 'start_page' : 31},\n",
    "    'itabirito' : {'filename' : 'plano-acao-climatica-itabirito.pdf', \"start_page\" : 19},\n",
    "    'joao_pessoa' : {'filename' : \"plano-acao-climatica-joao-pessoa.pdf\", \"start_page\" : 14},\n",
    "    'sao_paulo' : {'filename' : \"plano-acao-climatica-sp-regiao.pdf\", \"start_page\" : 11}, \n",
    "    'enfrentamento' : {'filename' : 'plano-enfrentamento-mudanca-climatica-nacional.pdf', \"start_page\" : 1} \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path: Path, output_txt_path: Path, start_page : int = 1, has_line_number : bool = False, skip_lines : int = 0):\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            full_text = \"\"\n",
    "            for page_number, page in enumerate(pdf.pages[start_page-1:], start=1):\n",
    "                # Extract text from the current page\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    # Optionally, add page breaks or headers\n",
    "                    full_text += f\"\\n\\n--- Page {page_number} ---\\n\\n\"\n",
    "                    if has_line_number:\n",
    "                        text = '\\n'.join([' '.join(line.split(' ')[1:]) for line in text.split('\\n')[skip_lines:]])\n",
    "                    elif skip_lines != 0:\n",
    "                        text = '\\n'.join(text.split('\\n')[skip_lines:])\n",
    "\n",
    "                    # remove last line from each page (contains the page number)\n",
    "                    full_text += '\\n'.join(text.split('\\n')[:-1])\n",
    "            # Write the extracted text to the output file\n",
    "            output_txt_path.write_text(full_text, encoding='utf-8')\n",
    "        print(f\"Text successfully extracted to {output_txt_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {pdf_path.name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text successfully extracted to c:\\Users\\esdra\\Documents\\BCG_Challenge\\data\\pre-processed\\nacional.txt\n",
      "Text successfully extracted to c:\\Users\\esdra\\Documents\\BCG_Challenge\\data\\pre-processed\\agro.txt\n",
      "Text successfully extracted to c:\\Users\\esdra\\Documents\\BCG_Challenge\\data\\pre-processed\\curitiba.txt\n",
      "Text successfully extracted to c:\\Users\\esdra\\Documents\\BCG_Challenge\\data\\pre-processed\\federal.txt\n",
      "Text successfully extracted to c:\\Users\\esdra\\Documents\\BCG_Challenge\\data\\pre-processed\\itabirito.txt\n",
      "Text successfully extracted to c:\\Users\\esdra\\Documents\\BCG_Challenge\\data\\pre-processed\\joao_pessoa.txt\n",
      "Text successfully extracted to c:\\Users\\esdra\\Documents\\BCG_Challenge\\data\\pre-processed\\sao_paulo.txt\n",
      "Text successfully extracted to c:\\Users\\esdra\\Documents\\BCG_Challenge\\data\\pre-processed\\enfretamento.txt\n"
     ]
    }
   ],
   "source": [
    "data_path = Path().cwd().parent / 'data'\n",
    "input_pdf_directory = data_path / 'raw'\n",
    "output_text_directory = data_path / 'pre-processed'\n",
    "\n",
    "output_text_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for name, metadata in metadata_pdfs.items():\n",
    "    txt_filename = name + '.txt'\n",
    "    output_txt_path = output_text_directory / txt_filename\n",
    "    extract_text_from_pdf(input_pdf_directory / metadata['filename'], output_txt_path, start_page = metadata['start_page'],has_line_number= 'line_number' in metadata, skip_lines= metadata['skip_lines'] if 'skip_lines' in metadata else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For the next step, meaningless strings from the documents will be removed, below a list of them for each doc\n",
    "strings_to_remove = {\n",
    "    'agro' : None,\n",
    "    'curitiba' : None,\n",
    "    'enfrentamento' : None,\n",
    "    'federal' : None,\n",
    "    'itabirito' : None,\n",
    "\n",
    "    'joao_pessoa' : [\"\"\"AOSSEP\n",
    "OÃOJ\n",
    "ED\n",
    "ACITÁMILC\n",
    "OÃÇA\n",
    "ED\n",
    "ONALP\"\"\",\n",
    "\"\"\"AOSSEP\n",
    "OÃOJ\n",
    "ED ACITÁMILC\n",
    "OÃÇA ED\n",
    "ONALP\"\"\"],\n",
    "\n",
    "    'nacional' : None,\n",
    "\n",
    "    'sao_paulo' : ['PARTE I - O PLANEJAMENTO DA ADAPTAÇÃO E DA RESILIÊNCIA',\n",
    "                               'PARTE II – CICLO DE ELABORAÇÃO DO PLANO',\n",
    "\"\"\"seõiger\n",
    "e\n",
    "soipícinum\n",
    "arap\n",
    "acitámilc\n",
    "aicnêiliser\n",
    "e\n",
    "oãçatpada\n",
    "ed\n",
    "aiuG\"\"\"],\n",
    "\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_txt_processed_dir = data_path / 'pre-processed'\n",
    "output_bronze_dir = data_path / 'bronze'\n",
    "\n",
    "output_bronze_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "for name, info in strings_to_remove.items():\n",
    "    output_text_path = output_bronze_dir / f'{name}.txt'\n",
    "    ## read the text from the file\n",
    "    text = (input_txt_processed_dir / (name + '.txt')).read_text(encoding='utf-8')\n",
    "    ## remove the strings\n",
    "    if info is not None:\n",
    "        for string in info:\n",
    "            text = text.replace(string, '')\n",
    "\n",
    "    ## remove the '--- Page X ---' strings\n",
    "    text = '\\n'.join([line for line in text.split('\\n') if not '--- Page' in line])\n",
    "\n",
    "    ## substitute the r'\\n+' for '\\n'\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "\n",
    "    ## write the text to the output file\n",
    "    output_text_path.write_text(text, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
