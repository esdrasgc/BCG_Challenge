from langchain_openai import ChatOpenAI
from langchain.prompts import (
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
    AIMessagePromptTemplate,
    ChatPromptTemplate
)
from langchain_core.output_parsers import StrOutputParser
import utils
import os
from dotenv import load_dotenv
import openai

load_dotenv()

openai_api_key = os.getenv("OPENAI_API_KEY")
langchain_api_key = os.getenv("LANGCHAIN_API_KEY")


openai.api_key = openai_api_key


def model_init(model_name: str = 'gpt-4o-mini', temperature:float = 0.2, max_tokens:int = None):
    model = ChatOpenAI(model_name=model_name, temperature=temperature, max_tokens=max_tokens)
    return model



def call_llm(query: str, model = None, k:int = 4, context = None, memory = None):
  """    
    Calls the LLM model to process the city planner's query.
    
    Args:
        query (str): The user query.
        model (ChatOpenAI): The instance of the model to be used.
        k (int): Number of documents to be returned (can be used later).
        context (list): Context to be used in the response.
        memory (str): Memory of the previous conversation.

    Returns:
        str: The response generated by the model.
"""


  if model is None:
    model = model_init()


  if context == None:
    docs = utils.retriever(query, k)
    context = [doc.page_content for doc in docs]   #rever isso pra implementar no grafo


  # Define a system prompt that guides the assistant to help a city planner with climate change-related planning
  # using official documents as context.
  if memory == None:
    memory = ' '
  system_prompt = (
          '''You are a helpful assistant, who will help a city planner on the theme of climate changes.
          Here is the chat memory, if it exists, use it:\n {memory}
          You will have official documents as context, use them to answer with quality and good foundation. \n
          contexte: \n {context}
      '''
      )

  # Create a SystemMessagePromptTemplate using the system_prompt.
  system_template = SystemMessagePromptTemplate.from_template(system_prompt)


  # Define a human prompt where the city planner provides a specific query for assistance.
  human_prompt = (
      '''I'm a city planner and want you assistance to do my planning on the climate changes.
      My job depends on this task. Here is my query: \n {query}'''
  )

  # Create a HumanMessagePromptTemplate using the human_prompt.
  human_template = HumanMessagePromptTemplate.from_template(human_prompt)

  # Combine the system and human prompts into a ChatPromptTemplate to guide the conversation.
  final_prompt = ChatPromptTemplate.from_messages(
    [
        system_template,
        human_template
    ]
  )

  # Create a chain that takes the final_prompt as input, uses the 'model' to process it, and parses the output with StrOutputParser.
  chain = (
      final_prompt  # Input prompt combining system and human messages
      | model       # Model processes the input prompt
      | StrOutputParser()  # Output is parsed as a string
  )
  # Execute the chain, using OpenAI's callback to track usage (like token count or cost).
  # The invoke method uses the 'context' and 'query' as inputs for the assistant to generate the response.
  
  output = chain.invoke({
      "context": context,  # Placeholder for the actual context (official documents, data, etc.)
      "query": query,    # Placeholder for the actual query from the city planner
      "memory": memory
  })
  return output