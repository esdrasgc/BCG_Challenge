from langchain_openai import ChatOpenAI
from langchain.prompts import (
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
    AIMessagePromptTemplate,
    ChatPromptTemplate
)
from langchain_core.output_parsers import StrOutputParser
import utils
import os
from dotenv import load_dotenv
import openai

load_dotenv()

openai_api_key = os.getenv("OPENAI_API_KEY")
langchain_api_key = os.getenv("LANGCHAIN_API_KEY")


openai.api_key = openai_api_key


def model_init(model_name: str = 'gpt-4o', temperature:float = 0.2, max_tokens:int = None):
    """
    Initialize the OpenAI model for the chatbot.

    Args:
        model_name (str): The name of the OpenAI model to be used.
        temperature (float): The temperature parameter for the model.
        max_tokens (int): The maximum number of tokens to be generated by the model.
      
    Returns:
        The ChatOpenAI instance of the model to be used.
    """
    model = ChatOpenAI(model_name=model_name, temperature=temperature, max_tokens=max_tokens)
    return model



def call_llm(query: str, model = None, k:int = 4, context = None, memory = None):
  """    
    Calls the LLM model to process the city planner's query.
    
    Args:
        query (str): The user query.
        model (ChatOpenAI): The instance of the model to be used.
        k (int): Number of documents to be returned (can be used later).
        context (list): Context to be used in the response.
        memory (str): Memory of the previous conversation.

    Returns:
        str: The response generated by the model.
"""


  if model is None:
    model = model_init()


  if context == None:
    context = ' '

  if memory == None:
    memory = ' '

  # Define a system prompt that guides the assistant to help a city planner with climate change-related planning
  system_prompt = (
          '''You are a helpful assistant, who will help a city planner on the theme of climate planning, who responds always in portuguese.
          Do not make a too long responses, just do long responses when the user asks for something like a summary of the conversation or like a final report. Avoid using numerical bullet points every time in normal prompts. The response should be a html string to render in the frontend.
          Here is the chat memory, if it exists, use it:\n {memory}.
          if in the history of the chat the user told you the name of his city, always keep this in mind for all the other responses.
          You will have official documents as context, use them to answer with quality and good foundation. \n
          context: \n {context}
      '''
      )

  # Create a SystemMessagePromptTemplate using the system_prompt.
  system_template = SystemMessagePromptTemplate.from_template(system_prompt)


  # Define a human prompt where the city planner provides a specific query for assistance.
  human_prompt = (
      '''I'm a city planner and want you assistance to do my planning on the climate planning.
      This task is crucial for my job. Here is my query: \n {query}'''
  )

  # Create a HumanMessagePromptTemplate using the human_prompt.
  human_template = HumanMessagePromptTemplate.from_template(human_prompt)

  # Combine the system and human prompts into a ChatPromptTemplate.
  final_prompt = ChatPromptTemplate.from_messages(
    [
        system_template,
        human_template
    ]
  )

  # Create a chain that takes the final_prompt as input, uses the 'model' to process it, and parses the output with StrOutputParser.
  chain = (
      final_prompt 
      | model      
      | StrOutputParser()  # Output is parsed as a string
  )

  
  # The invoke method uses the 'context', 'memor' and 'query' as inputs for the assistant to generate the response.
  output = chain.invoke({
      "context": context,  
      "query": query,    
      "memory": memory
  })
  return output